{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh4HVWjZVxSg"
      },
      "source": [
        "# Imports and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yPB4jtziVJbd"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchvision\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNOUwiuMGWaL",
        "outputId": "a4f0c93f-1486-4752-c0ec-c02edf002c83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\natha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\detection\\anchor_utils.py:63: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
            "  device: torch.device = torch.device(\"cpu\"),\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from PIL import Image as im\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EvjjlhySY4Iw"
      },
      "outputs": [],
      "source": [
        "def set_seed(worker_ID = 0):\n",
        "  # set seed.\n",
        "  worker_seed = (torch.initial_seed() + worker_ID) % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "def load_data(g, PATH_arr):\n",
        "  # load preprocessed data\n",
        "  X_train = np.load(PATH_arr[0])\n",
        "  y_train = np.load(PATH_arr[1])\n",
        "  mask_train = np.load(PATH_arr[2])\n",
        "  X_test = np.load(PATH_arr[3])\n",
        "  y_test = np.load(PATH_arr[4])\n",
        "  mask_test = np.load(PATH_arr[5])\n",
        "\n",
        "  # REFORMAT MASKS??\n",
        "  # How am I going to manage data\n",
        "\n",
        "  X_train_tensor = torch.Tensor(X_train)\n",
        "  mask_train_tensor = torch.Tensor(mask_train)\n",
        "  y_train_tensor = torch.Tensor(y_train)\n",
        "  X_test_tensor = torch.Tensor(X_test)\n",
        "  mask_test_tensor = torch.Tensor(mask_test)\n",
        "  y_test_tensor = torch.Tensor(y_test)\n",
        "\n",
        "  full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor, mask_train_tensor)\n",
        "  num_data = len(full_train_dataset) # may need to just hardcode\n",
        "  train_dataset, valid_dataset = random_split(full_train_dataset, [int(np.floor(num_data*0.8)), int(np.ceil(num_data*0.2))])\n",
        "  test_dataset = TensorDataset(X_test_tensor, y_test_tensor, mask_test_tensor)\n",
        "\n",
        "  # params to investigate: num_workers, pin_memory, worker_init_fn, generator\n",
        "  train_dataloader = DataLoader(train_dataset)\n",
        "  valid_dataloader = DataLoader(valid_dataset)\n",
        "  test_dataloader = DataLoader(test_dataset)\n",
        "  return train_dataloader, valid_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO later: DATA AUGMENTATION"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(save_top_k=3, monitor=\"val_loss\", mode=\"min\",filename=\"best_val-{epoch:02d}-val_loss:.2f\")\n",
        "model_dict = {}\n",
        "# to look over: torch.Generator, TensorBoardLogger params, EarlyStopping Params, pl.Trainer params\n",
        "\n",
        "def test(model_name, PATH_arr, seed):\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(seed)\n",
        "    train_dataloader, valid_dataloader, test_dataloader = load_data(g, PATH_arr)\n",
        "    logger = pl.loggers.TensorBoardLogger('test_logs', name = model_name)\n",
        "    model_naive = model_dict[model_name]\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
        "    trainer = pl.Trainer(max_epochs=300,logger=logger, \n",
        "                         callbacks=[early_stopping_callback, checkpoint_callback], \n",
        "                         auto_lr_find=True, accelerator=\"gpu\", devices=1, \n",
        "                         deterministic=True, gradient_clip_val=0.5)\n",
        "    trainer.fit(model_naive, train_dataloader, valid_dataloader)\n",
        "    return trainer, test_dataloader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import python model files\n",
        "\n",
        "PATH_arr = ['',\n",
        "            '',\n",
        "            '',\n",
        "            '',\n",
        "            '',\n",
        "            '']\n",
        "\n",
        "learning_rate = 3e-4\n",
        "model_dict = {\"model_1\": 1,\n",
        "              \"model_2\": 1,\n",
        "              \"model_3\": 1,\n",
        "              \"model_4\": 1,\n",
        "              \"model_5\": 1}\n",
        "\n",
        "model_name = \"model_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer, model, test_dataloader = test(model_name, PATH_arr, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.test(model, dataloaders=test_dataloader, ckpt_path=\"best\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir test_logs/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
